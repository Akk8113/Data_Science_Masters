{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOu9z58LOdsqjfyPzxTWoCn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","Q1. What is the KNN algorithm?\n","KNN is a simple, instance-based learning algorithm used for classification and regression tasks. It works by finding the 'k' nearest data points to a query point in the feature space and making predictions based on these neighbors. For classification, it assigns the class most common among the k neighbors, and for regression, it averages the values of the k neighbors.\n","\n","Q2. How do you choose the value of K in KNN?\n","The value of K is chosen through techniques like cross-validation, where different values are tested and the one that performs best on validation data is selected. A small K can lead to a model that is too sensitive to noise (overfitting), while a large K can make the model too generalized (underfitting). The ideal K balances bias and variance.\n","\n","Q3. What is the difference between KNN classifier and KNN regressor?\n","\n","KNN Classifier: Predicts the class label for a data point based on the majority class among its k nearest neighbors.\n","KNN Regressor: Predicts the continuous value for a data point by averaging the values of its k nearest neighbors.\n","Q4. How do you measure the performance of KNN?\n","\n","For Classification: Accuracy, precision, recall, F1 score, and confusion matrix.\n","For Regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and RÂ² score.\n","Q5. What is the curse of dimensionality in KNN?\n","The curse of dimensionality refers to the problem where the feature space becomes sparse as the number of dimensions (features) increases. In high-dimensional spaces, distances between points become less meaningful, which can degrade the performance of KNN by making it hard to find truly \"nearest\" neighbors.\n","\n","Q6. How do you handle missing values in KNN?\n","\n","Imputation: Replace missing values with statistical estimates (mean, median, mode) or use models specifically designed for imputation.\n","Weighted Distance: Modify distance calculations to handle missing values by adjusting the contribution of features with missing values.\n","Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n","\n","KNN Classifier: Best for problems where you need to classify items into categories. It performs well when the decision boundary is irregular and complex.\n","KNN Regressor: Best for problems where you need to predict a continuous value. It is useful when relationships between features and target values are non-linear and complex.\n","Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n","\n","Strengths: Simple to understand and implement; effective with a small number of features; non-parametric.\n","Weaknesses: Computationally expensive for large datasets; performance degrades with high dimensionality; sensitive to irrelevant or redundant features.\n","Addressing Weaknesses: Use dimensionality reduction techniques (like PCA), feature selection, and efficient data structures (like KD-trees) for large datasets.\n","Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n","\n","Euclidean Distance: Measures the straight-line distance between two points in Euclidean space. It is computed as the square root of the sum of squared differences in each dimension.\n","Manhattan Distance: Measures the distance between two points by summing the absolute differences of their coordinates. It is akin to the distance you would travel on a grid-based path.\n","Q10. What is the role of feature scaling in KNN?\n","Feature scaling is crucial in KNN because the algorithm relies on distance calculations. Without scaling, features with larger ranges can dominate the distance measure, leading to biased results. Standardizing or normalizing features ensures that all features contribute equally to the distance calculations."],"metadata":{"id":"1iQtqGj3Gngc"}},{"cell_type":"code","source":[],"metadata":{"id":"q4-j2qA7G5IR"},"execution_count":null,"outputs":[]}]}