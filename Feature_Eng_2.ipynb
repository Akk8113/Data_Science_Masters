{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOcRwxPCN3oiciQq768xlIM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1: Filter Method in Feature Selection\n","Definition: The Filter method selects features based on their statistical properties and relevance to the target variable before the modeling process. It is independent of the learning algorithm.\n","\n","How It Works:\n","\n","Statistical Tests: Measures the relationship between each feature and the target variable using statistical tests (e.g., Chi-square test, ANOVA).\n","Correlation Analysis: Evaluates the correlation between features and the target variable.\n","Ranking: Features are ranked based on their scores or p-values, and a subset of the top-ranked features is selected.\n","Example:\n","\n","python\n","Copy code\n","from sklearn.feature_selection import SelectKBest, f_classif\n","import pandas as pd\n","\n","# Sample data\n","X = pd.DataFrame({\n","    'feature1': [1, 2, 3, 4, 5],\n","    'feature2': [2, 3, 4, 5, 6],\n","    'feature3': [5, 6, 7, 8, 9]\n","})\n","y = [0, 1, 0, 1, 0]\n","\n","# Apply Filter method\n","selector = SelectKBest(score_func=f_classif, k=2)\n","X_new = selector.fit_transform(X, y)\n","Q2: Wrapper Method vs. Filter Method\n","Wrapper Method:\n","\n","Definition: The Wrapper method evaluates feature subsets by training and testing the model using different feature subsets and selecting the best-performing one based on model performance.\n","How It Works: Uses algorithms like forward selection, backward elimination, or recursive feature elimination.\n","Advantages: Can account for feature interactions and model-specific requirements.\n","Disadvantages: Computationally expensive and may lead to overfitting.\n","Filter Method:\n","\n","Definition: Selects features based on statistical measures or tests before model training.\n","How It Works: Uses statistical techniques to evaluate individual features' relevance without involving the learning algorithm.\n","Advantages: Computationally efficient and less prone to overfitting.\n","Disadvantages: Ignores feature interactions and may not capture complex relationships.\n","Q3: Common Techniques in Embedded Feature Selection Methods\n","1. Lasso (L1 Regularization):\n","\n","Adds a penalty proportional to the absolute value of coefficients.\n","Can shrink some coefficients to zero, effectively performing feature selection.\n","Example:\n","python\n","Copy code\n","from sklearn.linear_model import Lasso\n","\n","model = Lasso(alpha=0.01)\n","model.fit(X_train, y_train)\n","selected_features = X_train.columns[model.coef_ != 0]\n","2. Decision Trees/Random Forests:\n","\n","Measures feature importance based on how well features split the data in decision trees.\n","Example:\n","python\n","Copy code\n","from sklearn.ensemble import RandomForestClassifier\n","\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","importances = model.feature_importances_\n","3. Gradient Boosting Machines (GBM):\n","\n","Uses boosting to improve model accuracy and feature selection.\n","Example:\n","python\n","Copy code\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","model = GradientBoostingClassifier()\n","model.fit(X_train, y_train)\n","importances = model.feature_importances_\n","Q4: Drawbacks of the Filter Method\n","Independence: Does not consider feature interactions and relationships with other features.\n","Feature Relationships: May miss features that have predictive power when combined with other features.\n","Over-Simplification: Relies on univariate metrics, which may not capture complex interactions.\n","Q5: When to Use the Filter Method Over the Wrapper Method\n","When to Prefer Filter Method:\n","\n","High Dimensionality: When the dataset has a large number of features, making Wrapper methods computationally expensive.\n","Quick Evaluation: When a quick and computationally efficient feature selection is needed.\n","Simplicity: When the relationship between features and the target variable is relatively simple and linear.\n","Q6: Choosing Pertinent Attributes Using the Filter Method for Customer Churn\n","Steps:\n","\n","Statistical Tests: Use statistical tests (e.g., Chi-square, ANOVA) to assess the relationship between each feature and the churn variable.\n","Correlation Analysis: Evaluate the correlation between each feature and the churn status.\n","Select Features: Choose features with the highest scores or most significant p-values.\n","Example Code:\n","python\n","Copy code\n","from sklearn.feature_selection import SelectKBest, chi2\n","\n","# Assume df is the DataFrame and 'churn' is the target variable\n","X = df.drop('churn', axis=1)\n","y = df['churn']\n","\n","# Apply Filter method\n","selector = SelectKBest(score_func=chi2, k=5)\n","X_new = selector.fit_transform(X, y)\n","selected_features = X.columns[selector.get_support()]\n","Q7: Using Embedded Method for Soccer Match Prediction\n","Steps:\n","\n","Train a Model: Train a model that supports feature importance (e.g., Decision Tree, Random Forest).\n","Evaluate Feature Importance: Assess the importance of each feature based on the model.\n","Select Features: Choose features with the highest importance scores.\n","Example Code:\n","python\n","Copy code\n","from sklearn.ensemble import RandomForestClassifier\n","\n","model = RandomForestClassifier()\n","model.fit(X_train, y_train)\n","importances = model.feature_importances_\n","\n","# Select top features\n","important_features = X_train.columns[importances > threshold]\n","Q8: Using Wrapper Method for House Price Prediction\n","Steps:\n","\n","Choose a Wrapper Method: Use forward selection, backward elimination, or recursive feature elimination.\n","Train and Evaluate: Train models using different feature subsets and evaluate performance.\n","Select Best Features: Choose the subset of features that results in the best model performance.\n","Example Code:\n","python\n","Copy code\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LinearRegression\n","\n","model = LinearRegression()\n","rfe = RFE(model, n_features_to_select=5)\n","fit = rfe.fit(X_train, y_train)\n","\n","# Selected features\n","selected_features = X_train.columns[fit.support_]\n","These methods help in selecting the most relevant features for improving model performance and interpretability."],"metadata":{"id":"A7MHISAc0h5X"}}]}