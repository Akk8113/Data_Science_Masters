{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6ThEbWHkSYFm8QW+ef955"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. Probability of Being a Smoker Given Health Insurance Use\n","To find the probability that an employee is a smoker given that they use the company's health insurance plan, use Bayes' theorem:\n","\n","𝑃\n","(\n","Smoker\n","∣\n","Health Insurance\n",")\n","=\n","𝑃\n","(\n","Health Insurance\n","∣\n","Smoker\n",")\n","⋅\n","𝑃\n","(\n","Smoker\n",")\n","𝑃\n","(\n","Health Insurance\n",")\n","P(Smoker∣Health Insurance)=\n","P(Health Insurance)\n","P(Health Insurance∣Smoker)⋅P(Smoker)\n","​\n","\n","\n","Given:\n","\n","𝑃\n","(\n","Health Insurance\n",")\n","=\n","0.70\n","P(Health Insurance)=0.70\n","𝑃\n","(\n","Smoker\n","∣\n","Health Insurance\n",")\n","=\n","0.40\n","P(Smoker∣Health Insurance)=0.40\n","We need to find\n","𝑃\n","(\n","Smoker\n",")\n","P(Smoker), which can be deduced from the information given. Let’s assume the probability of being a smoker is the same regardless of whether they use the health insurance or not, so:\n","\n","𝑃\n","(\n","Smoker\n",")\n","=\n","𝑃\n","(\n","Smoker\n","∣\n","Health Insurance\n",")\n","⋅\n","𝑃\n","(\n","Health Insurance\n",")\n","=\n","0.40\n","⋅\n","0.70\n","=\n","0.28\n","P(Smoker)=P(Smoker∣Health Insurance)⋅P(Health Insurance)=0.40⋅0.70=0.28\n","\n","We then calculate\n","𝑃\n","(\n","Smoker\n","∣\n","Health Insurance\n",")\n","P(Smoker∣Health Insurance) as:\n","\n","𝑃\n","(\n","Smoker\n","∣\n","Health Insurance\n",")\n","=\n","0.40\n","⋅\n","0.70\n","0.70\n","=\n","0.40\n","P(Smoker∣Health Insurance)=\n","0.70\n","0.40⋅0.70\n","​\n"," =0.40\n","\n","So, the probability is 0.40 or 40%.\n","\n","Q2. Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n","Bernoulli Naive Bayes: Assumes binary (0 or 1) features. It is suited for binary/boolean features where features are either present or absent.\n","Multinomial Naive Bayes: Assumes features represent counts or frequencies. It is suited for discrete features where the frequency of occurrence matters, such as word counts in text classification.\n","Q3. Handling Missing Values in Bernoulli Naive Bayes\n","Bernoulli Naive Bayes does not inherently handle missing values. Common approaches to handle missing values before applying Bernoulli Naive Bayes include:\n","\n","Imputation: Replace missing values with a constant value or statistical measures (mean, median).\n","Feature Engineering: Create an additional binary feature to indicate missingness.\n","Q4. Gaussian Naive Bayes for Multi-class Classification\n","Yes, Gaussian Naive Bayes can be used for multi-class classification. It assumes that the features follow a Gaussian (normal) distribution and calculates the probability of each class given the feature values. It works well for continuous features and can handle multiple classes by applying the same principles used for binary classification.\n","\n","Q5. Assignment\n","Data Preparation:\n","Download the dataset from the UCI Machine Learning Repository.\n","Load the dataset and perform necessary preprocessing steps, such as handling missing values, normalization, and splitting the data into training and testing sets.\n","Implementation:\n","Implement Bernoulli Naive Bayes:\n","\n","python\n","Copy code\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.model_selection import cross_val_score\n","import pandas as pd\n","\n","# Load dataset\n","df = pd.read_csv('spambase.data', header=None)\n","X = df.iloc[:, :-1]\n","y = df.iloc[:, -1]\n","\n","# Initialize and evaluate Bernoulli Naive Bayes\n","bernoulli_nb = BernoulliNB()\n","scores = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n","Implement Multinomial Naive Bayes:\n","\n","python\n","Copy code\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# Initialize and evaluate Multinomial Naive Bayes\n","multinomial_nb = MultinomialNB()\n","scores = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n","Implement Gaussian Naive Bayes:\n","\n","python\n","Copy code\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Initialize and evaluate Gaussian Naive Bayes\n","gaussian_nb = GaussianNB()\n","scores = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n","Compute performance metrics (Accuracy, Precision, Recall, F1 score):\n","\n","python\n","Copy code\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# After fitting the models and making predictions\n","y_pred = gaussian_nb.predict(X_test)\n","\n","accuracy = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy').mean()\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","Results and Discussion:\n","Report the metrics for each classifier: Accuracy, Precision, Recall, F1 score.\n","Discuss the results:\n","Which classifier performed best?\n","Why did it perform better or worse compared to others?\n","Consider the nature of the data and how each Naive Bayes variant handles it.\n","Conclusion:\n","Summarize findings from the performance evaluation.\n","Suggest future work:\n","Experiment with additional preprocessing steps.\n","Try other classification algorithms for comparison.\n","Investigate feature selection techniques to improve model performance."],"metadata":{"id":"1nPwB8higcBr"}},{"cell_type":"code","source":[],"metadata":{"id":"DMqZ8uJ4geqg"},"execution_count":null,"outputs":[]}]}