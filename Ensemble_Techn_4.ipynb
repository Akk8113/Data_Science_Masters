{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwbmGs+wVUBXFVbcGpx/6m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1: Design a Pipeline for Feature Engineering and Random Forest Classifier\n","Let's create a pipeline that includes automated feature selection, imputation, scaling, and model training using a Random Forest Classifier. Here's how we can achieve this step-by-step:\n","\n","Automated Feature Selection: We'll use SelectKBest with a statistical test (e.g., chi-squared for classification tasks) to select the most important features.\n","Numerical Pipeline:\n","Impute missing values using the mean.\n","Scale the numerical features using standardization.\n","Categorical Pipeline:\n","Impute missing values using the most frequent value.\n","One-hot encode the categorical features.\n","Combine Pipelines: Use ColumnTransformer to apply the numerical and categorical pipelines to the respective columns.\n","Random Forest Classifier: Train the final model using a Random Forest Classifier.\n","Evaluate Model: Evaluate the model's performance on a test set.\n","Here's the implementation:\n","\n","python\n","Copy code\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","url = 'your_dataset_url'\n","data = pd.read_csv(url)\n","\n","# Define feature columns and target\n","target = 'target_column'\n","features = [col for col in data.columns if col != target]\n","\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.3, random_state=42)\n","\n","# Identify numerical and categorical columns\n","numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n","categorical_cols = X_train.select_dtypes(include=['object']).columns\n","\n","# Numerical pipeline\n","numerical_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='mean')),\n","    ('scaler', StandardScaler())\n","])\n","\n","# Categorical pipeline\n","categorical_pipeline = Pipeline([\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Combine numerical and categorical pipelines\n","preprocessor = ColumnTransformer([\n","    ('num', numerical_pipeline, numerical_cols),\n","    ('cat', categorical_pipeline, categorical_cols)\n","])\n","\n","# Automated feature selection and model pipeline\n","pipeline = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('feature_selection', SelectKBest(score_func=chi2, k=10)),  # Select top 10 features\n","    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n","])\n","\n","# Train the model\n","pipeline.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy:.4f}')\n","Explanation of Each Step:\n","\n","Automated Feature Selection: SelectKBest selects the top 10 features based on chi-squared statistics.\n","Numerical Pipeline: Imputes missing values with the mean and standardizes the features.\n","Categorical Pipeline: Imputes missing values with the most frequent value and one-hot encodes the features.\n","Combine Pipelines: ColumnTransformer applies the numerical and categorical pipelines to their respective columns.\n","Random Forest Classifier: Trains a Random Forest model with 100 trees and a maximum depth of 10.\n","Model Evaluation: Evaluates the model's accuracy on the test set.\n","Interpretation of Results and Possible Improvements:\n","\n","The accuracy score provides an initial indication of model performance.\n","To improve the pipeline, consider:\n","Hyperparameter tuning of the Random Forest model.\n","Experimenting with different feature selection methods.\n","Incorporating other preprocessing steps, such as outlier removal or feature engineering.\n","Q2: Build a Pipeline with Voting Classifier\n","Let's build a pipeline that combines a Random Forest Classifier and a Logistic Regression Classifier using a Voting Classifier. We'll train and evaluate this pipeline on the Iris dataset.\n","\n","python\n","Copy code\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import VotingClassifier\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define classifiers\n","clf1 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n","clf2 = LogisticRegression(max_iter=200)\n","\n","# Voting classifier\n","voting_clf = VotingClassifier(estimators=[\n","    ('rf', clf1),\n","    ('lr', clf2)\n","], voting='hard')\n","\n","# Create pipeline\n","voting_pipeline = Pipeline([\n","    ('classifier', voting_clf)\n","])\n","\n","# Train the model\n","voting_pipeline.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = voting_pipeline.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Voting Classifier Accuracy: {accuracy:.4f}')\n","This code combines predictions from a Random Forest and Logistic Regression using hard voting, where the final prediction is based on the majority vote. It provides an ensemble approach that can leverage the strengths of both classifiers.\n","\n","Summary of Steps:\n","\n","Load and split the Iris dataset.\n","Define the Random Forest and Logistic Regression classifiers.\n","Combine them using a Voting Classifier.\n","Train and evaluate the model using the pipeline.\n","Interpretation:\n","\n","The accuracy score gives an indication of the ensemble's performance.\n","Improvements could include tuning hyperparameters, using soft voting, or including more diverse classifiers.\n","Feel free to ask if you have any further questions or need additional clarifications!"],"metadata":{"id":"4vKndJOTEIiw"}},{"cell_type":"code","source":[],"metadata":{"id":"iOf2DT_HEVGi"},"execution_count":null,"outputs":[]}]}