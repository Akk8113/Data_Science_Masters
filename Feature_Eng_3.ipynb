{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/LC6o0Y3JNY2WdjGFk6rc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1: Min-Max Scaling\n","Min-Max Scaling is a data normalization technique that transforms features to a common scale, typically [0, 1]. This is done by scaling the data based on the minimum and maximum values of the feature.\n","\n","Formula:\n","X\n","scaled\n","=\n","𝑋\n","−\n","X\n","min\n","X\n","max\n","−\n","X\n","min\n","X\n","scaled\n","​\n"," =\n","X\n","max\n","​\n"," −X\n","min\n","​\n","\n","X−X\n","min\n","​\n","\n","​\n","\n","\n","Example:\n","\n","Consider a feature with values [1, 5, 10, 15, 20]. We want to scale these values to the range [0, 1].\n","\n","Find Min and Max:\n","\n","Min = 1\n","Max = 20\n","Apply Min-Max Scaling:\n","\n","For 1:\n","1\n","−\n","1\n","20\n","−\n","1\n","=\n","0\n","20−1\n","1−1\n","​\n"," =0\n","For 5:\n","5\n","−\n","1\n","20\n","−\n","1\n","≈\n","0.211\n","20−1\n","5−1\n","​\n"," ≈0.211\n","For 10:\n","10\n","−\n","1\n","20\n","−\n","1\n","≈\n","0.474\n","20−1\n","10−1\n","​\n"," ≈0.474\n","For 15:\n","15\n","−\n","1\n","20\n","−\n","1\n","≈\n","0.737\n","20−1\n","15−1\n","​\n"," ≈0.737\n","For 20:\n","20\n","−\n","1\n","20\n","−\n","1\n","=\n","1\n","20−1\n","20−1\n","​\n"," =1\n","The scaled values are approximately [0, 0.211, 0.474, 0.737, 1].\n","\n","Q2: Unit Vector Technique\n","The Unit Vector Technique, also known as Normalization or Vector Normalization, scales the features to have a unit norm (length of 1). This is often used in machine learning when features need to be on the same scale but is different from Min-Max scaling because it does not constrain the data to a specific range.\n","\n","Formula:\n","X\n","normalized\n","=\n","X\n","∥\n","X\n","∥\n","X\n","normalized\n","​\n"," =\n","∥X∥\n","X\n","​\n","\n","Where\n","∥\n","X\n","∥\n","∥X∥ is the norm of the vector.\n","\n","Example:\n","\n","Consider a vector [3, 4].\n","\n","Calculate Norm:\n","∥\n","X\n","∥\n","=\n","3\n","2\n","+\n","4\n","2\n","=\n","9\n","+\n","16\n","=\n","5\n","∥X∥=\n","3\n","2\n"," +4\n","2\n","\n","​\n"," =\n","9+16\n","​\n"," =5\n","\n","Normalize:\n","\n","For 3:\n","3\n","5\n","=\n","0.6\n","5\n","3\n","​\n"," =0.6\n","For 4:\n","4\n","5\n","=\n","0.8\n","5\n","4\n","​\n"," =0.8\n","The normalized vector is [0.6, 0.8].\n","\n","Difference from Min-Max Scaling:\n","\n","Min-Max Scaling transforms features to a specific range [0, 1] or [-1, 1].\n","Unit Vector Normalization scales the data to have a unit norm but does not constrain it to a specific range.\n","Q3: Principal Component Analysis (PCA)\n","PCA is a dimensionality reduction technique that transforms data into a new coordinate system where the greatest variances by any projection of the data come to lie on the first coordinates called principal components.\n","\n","Example:\n","\n","Consider a dataset with two features: height and weight.\n","\n","Standardize the data (subtract mean, divide by standard deviation).\n","Compute the covariance matrix of the standardized data.\n","Calculate eigenvalues and eigenvectors of the covariance matrix.\n","Sort the eigenvalues and choose the top\n","𝑘\n","k eigenvectors (principal components) that capture the most variance.\n","Transform the data using these principal components.\n","If we retain 2 principal components, we transform our 2D data into a new coordinate system where the axes represent the directions of maximum variance.\n","\n","Q4: PCA and Feature Extraction\n","PCA can be used for Feature Extraction by selecting the most significant principal components that capture the most variance in the dataset.\n","\n","Example:\n","\n","Suppose we have features: [height, weight, age]. PCA could reduce these three features to two principal components.\n","\n","Choose Components: If the first two principal components capture 90% of the variance, you might choose these two components.\n","Transform Data: The original features (height, weight, age) are projected onto these two principal components, resulting in two new features that are combinations of the original ones.\n","Q5: Min-Max Scaling for a Recommendation System\n","For a recommendation system with features like price, rating, and delivery time:\n","\n","Apply Min-Max Scaling to each feature:\n","Scale price to the range [0, 1] to ensure comparability.\n","Scale rating and delivery time similarly.\n","Python Example:\n","\n","python\n","Copy code\n","from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","\n","data = pd.DataFrame({\n","    'price': [10, 20, 30, 40],\n","    'rating': [4.5, 3.5, 5.0, 4.0],\n","    'delivery_time': [30, 45, 20, 60]\n","})\n","\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data)\n","\n","print(scaled_data)\n","Q6: PCA for Dimensionality Reduction in Stock Prices\n","For a dataset with many features:\n","\n","Standardize the dataset.\n","Apply PCA to reduce the number of features while retaining most of the variance.\n","Python Example:\n","\n","python\n","Copy code\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","\n","data = pd.DataFrame({...})  # Your dataset with many features\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data)\n","\n","pca = PCA(n_components=5)  # Number of components to keep\n","reduced_data = pca.fit_transform(scaled_data)\n","\n","print(reduced_data)\n","Q7: Min-Max Scaling Example\n","Data: [1, 5, 10, 15, 20]\n","\n","Find Min and Max:\n","\n","Min = 1\n","Max = 20\n","Apply Scaling to range [-1, 1]:\n","\n","For 1:\n","2\n","×\n","(\n","1\n","−\n","1\n",")\n","20\n","−\n","1\n","−\n","1\n","=\n","−\n","1\n","20−1\n","2×(1−1)\n","​\n"," −1=−1\n","For 5:\n","2\n","×\n","(\n","5\n","−\n","1\n",")\n","20\n","−\n","1\n","−\n","1\n","≈\n","−\n","0.579\n","20−1\n","2×(5−1)\n","​\n"," −1≈−0.579\n","For 10:\n","2\n","×\n","(\n","10\n","−\n","1\n",")\n","20\n","−\n","1\n","−\n","1\n","≈\n","0\n","20−1\n","2×(10−1)\n","​\n"," −1≈0\n","For 15:\n","2\n","×\n","(\n","15\n","−\n","1\n",")\n","20\n","−\n","1\n","−\n","1\n","≈\n","0.579\n","20−1\n","2×(15−1)\n","​\n"," −1≈0.579\n","For 20:\n","2\n","×\n","(\n","20\n","−\n","1\n",")\n","20\n","−\n","1\n","−\n","1\n","=\n","1\n","20−1\n","2×(20−1)\n","​\n"," −1=1\n","The transformed values are approximately [-1, -0.579, 0, 0.579, 1].\n","\n","Q8: Feature Extraction with PCA\n","Dataset: [height, weight, age, gender, blood pressure]\n","\n","Preprocessing: Convert categorical features to numerical.\n","Standardize the dataset.\n","Apply PCA to extract features:\n","Choose Principal Components: Select the number of principal components that capture a significant amount of variance (e.g., 95%).\n","Python Example:\n","\n","python\n","Copy code\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","\n","data = pd.DataFrame({...})  # Your dataset with features\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data)\n","\n","pca = PCA(n_components=3)  # Choose number of components\n","pca_data = pca.fit_transform(scaled_data)\n","\n","print(pca_data)\n","Principal Components Selection:\n","Choose the number of components based on the cumulative explained variance ratio to retain a desired amount of variance (e.g., 95%).\n","\n","These techniques and concepts help in preprocessing and reducing the dimensionality of data, which improves model performance and interpretability."],"metadata":{"id":"O155ErLe1CEE"}}]}