{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWMlXF5vP/7+r+deq4TH4U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1: Overfitting and Underfitting\n","Overfitting:\n","Overfitting occurs when a machine learning model learns the training data too well, including its noise and outliers, which leads to poor generalization to new, unseen data. The model becomes too complex, capturing the idiosyncrasies of the training data rather than the underlying patterns.\n","\n","Consequences of Overfitting:\n","\n","High accuracy on training data but poor performance on test data.\n","The model is sensitive to small fluctuations in the training data.\n","Mitigation of Overfitting:\n","\n","Cross-validation: Use techniques like k-fold cross-validation to ensure the model generalizes well.\n","Pruning: In decision trees, remove branches that have little importance.\n","Regularization: Add penalties to the loss function to discourage overly complex models (e.g., L1 and L2 regularization).\n","Reducing complexity: Simplify the model by reducing the number of features or choosing a less complex algorithm.\n","Increase training data: More data can help the model learn the general patterns rather than memorizing the training set.\n","Underfitting:\n","Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. The model fails to learn the relationships between the input features and the target variable.\n","\n","Consequences of Underfitting:\n","\n","Poor performance on both training and test data.\n","The model misses significant trends or patterns in the data.\n","Mitigation of Underfitting:\n","\n","Increase model complexity: Use more complex models or algorithms capable of capturing more intricate patterns.\n","Feature engineering: Add more relevant features or use feature transformations to improve the model's ability to learn.\n","Reduce regularization: Too much regularization can constrain the model too much, leading to underfitting.\n","Q2: Reducing Overfitting\n","Regularization: Adding a penalty for larger weights in the model to the loss function. Common techniques include L1 (Lasso) and L2 (Ridge) regularization.\n","Cross-validation: Using techniques like k-fold cross-validation helps to ensure that the model's performance is consistent across different subsets of the data.\n","Pruning: In decision trees, reduce the size of the tree by removing branches that provide little power to the model.\n","Reduce model complexity: Use simpler models or reduce the number of parameters.\n","Data augmentation: Increase the size of the training data by adding modified versions of existing data.\n","Early stopping: Monitor the model's performance on a validation set during training and stop the training process once the performance starts to degrade.\n","Q3: Underfitting\n","Underfitting:\n","Underfitting occurs when the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets.\n","\n","Scenarios Where Underfitting Can Occur:\n","\n","Too Simple Model: Using a linear model to fit data with a non-linear relationship.\n","Insufficient Training Time: Stopping training too early in algorithms like neural networks, before the model has fully learned the"],"metadata":{"id":"3T8eQ8nmzm6L"}}]}