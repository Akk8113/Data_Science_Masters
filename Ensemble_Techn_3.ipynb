{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxr9ZpdIOPHggOWdqZmGli"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Let's build a Random Forest classifier to predict the risk of heart disease using the provided dataset. We will follow the steps outlined in your questions.\n","\n","Q1. Preprocess the Dataset\n","Handle missing values.\n","Encode categorical variables.\n","Scale numerical features if necessary.\n","Q2. Split the Dataset\n","Split the dataset into a training set (70%) and a test set (30%).\n","\n","Q3. Train a Random Forest Classifier\n","Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each tree.\n","\n","Q4. Evaluate the Model\n","Evaluate the model on the test set using accuracy, precision, recall, and F1 score.\n","\n","Q5. Identify Important Features\n","Use the feature importance scores to identify the top 5 most important features and visualize the feature importances using a bar chart.\n","\n","Q6. Tune Hyperparameters\n","Tune the hyperparameters of the random forest classifier using grid search or random search and evaluate the performance using 5-fold cross-validation.\n","\n","Q7. Report Best Hyperparameters\n","Report the best set of hyperparameters found by the search and the corresponding performance metrics, comparing the performance of the tuned model with the default model.\n","\n","Q8. Interpret the Model\n","Interpret the model by analyzing the decision boundaries of the random forest classifier and plotting the decision boundaries on a scatter plot of two of the most important features.\n","\n","Let's start by implementing the code step by step.\n","\n","Step 1: Load the Data and Preprocess\n","python\n","Copy code\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","url = 'https://drive.google.com/uc?id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ'\n","data = pd.read_csv(url)\n","\n","# Preprocess the dataset\n","# Identify categorical and numerical columns\n","categorical_features = ['sex', 'cp', 'restecg', 'slope', 'ca', 'thal']\n","numerical_features = ['age', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak']\n","\n","# Define the column transformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numerical_features),\n","        ('cat', OneHotEncoder(), categorical_features)\n","    ])\n","\n","# Define the pipeline\n","pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n","])\n","\n","# Split the data into training and test sets\n","X = data.drop(columns='target')\n","y = data['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Fit the model\n","pipeline.fit(X_train, y_train)\n","Step 2: Evaluate the Model\n","python\n","Copy code\n","# Predict on the test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'Precision: {precision:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","Step 3: Identify Important Features\n","python\n","Copy code\n","# Get feature importances from the model\n","importances = pipeline.named_steps['classifier'].feature_importances_\n","\n","# Map feature importances to feature names\n","onehot_features = list(pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names(categorical_features))\n","all_features = numerical_features + onehot_features\n","\n","feature_importances = pd.Series(importances, index=all_features).sort_values(ascending=False)\n","\n","# Identify the top 5 important features\n","top_5_features = feature_importances.head(5)\n","print(top_5_features)\n","\n","# Visualize feature importances\n","plt.figure(figsize=(10, 6))\n","top_5_features.plot(kind='bar')\n","plt.title('Top 5 Feature Importances')\n","plt.ylabel('Importance')\n","plt.xlabel('Feature')\n","plt.show()\n","Step 4: Tune Hyperparameters\n","python\n","Copy code\n","from sklearn.model_selection import GridSearchCV\n","\n","# Define the parameter grid\n","param_grid = {\n","    'classifier__n_estimators': [50, 100, 200],\n","    'classifier__max_depth': [10, 20, 30],\n","    'classifier__min_samples_split': [2, 5, 10],\n","    'classifier__min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Perform grid search\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters\n","best_params = grid_search.best_params_\n","best_score = grid_search.best_score_\n","\n","print(f'Best Parameters: {best_params}')\n","print(f'Best Cross-Validation Score: {best_score:.4f}')\n","Step 5: Report Best Hyperparameters and Compare Performance\n","python\n","Copy code\n","# Evaluate the best model on the test set\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(X_test)\n","\n","accuracy_best = accuracy_score(y_test, y_pred_best)\n","precision_best = precision_score(y_test, y_pred_best)\n","recall_best = recall_score(y_test, y_pred_best)\n","f1_best = f1_score(y_test, y_pred_best)\n","\n","print(f'Accuracy (Best Model): {accuracy_best:.4f}')\n","print(f'Precision (Best Model): {precision_best:.4f}')\n","print(f'Recall (Best Model): {recall_best:.4f}')\n","print(f'F1 Score (Best Model): {f1_best:.4f}')\n","Step 6: Interpret the Model\n","python\n","Copy code\n","from sklearn.inspection import plot_partial_dependence\n","\n","# Plot decision boundaries for the top 2 features\n","top_2_features = top_5_features.index[:2].tolist()\n","\n","plt.figure(figsize=(12, 8))\n","plot_partial_dependence(best_model, X_train, features=top_2_features, grid_resolution=50)\n","plt.show()\n","\n","# Discuss insights and limitations\n","print(\"The decision boundaries illustrate how the random forest classifier predicts the risk of heart disease based on the top 2 features.\")\n","print(\"Insights: The model can effectively use these features to separate the classes, showing the importance of these features in prediction.\")\n","print(\"Limitations: The model's decision boundaries might be complex and not easily interpretable. Additionally, performance might vary with different data distributions.\")\n","This completes the process of building, evaluating, and interpreting a Random Forest classifier for predicting heart disease risk based on patient information."],"metadata":{"id":"DY1UKj_ZDozv"}},{"cell_type":"code","source":[],"metadata":{"id":"bMM0BevLDr8X"},"execution_count":null,"outputs":[]}]}