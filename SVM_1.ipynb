{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNo95DgsX8/7LGHvpZXiKL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. What is the Mathematical Formula for a Linear SVM?\n","The mathematical formula for a linear Support Vector Machine (SVM) involves finding a hyperplane that best separates the data into two classes. The decision boundary or hyperplane can be represented as:\n","\n","𝑤\n","⋅\n","𝑥\n","+\n","𝑏\n","=\n","0\n","w⋅x+b=0\n","\n","where:\n","\n","𝑤\n","w is the weight vector,\n","𝑥\n","x is the feature vector,\n","𝑏\n","b is the bias term.\n","The goal of the linear SVM is to find the hyperplane that maximizes the margin between the two classes.\n","\n","Q2. What is the Objective Function of a Linear SVM?\n","The objective function of a linear SVM is to find the hyperplane that maximizes the margin between the support vectors of the two classes while minimizing the classification error. The objective can be formulated as:\n","\n","min\n","⁡\n","𝑤\n",",\n","𝑏\n","1\n","2\n","∥\n","𝑤\n","∥\n","2\n","+\n","𝐶\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","𝜉\n","𝑖\n","min\n","w,b\n","​\n","  \n","2\n","1\n","​\n"," ∥w∥\n","2\n"," +C∑\n","i=1\n","n\n","​\n"," ξ\n","i\n","​\n","\n","\n","subject to the constraints:\n","\n","𝑦\n","𝑖\n","(\n","𝑤\n","⋅\n","𝑥\n","𝑖\n","+\n","𝑏\n",")\n","≥\n","1\n","−\n","𝜉\n","𝑖\n","y\n","i\n","​\n"," (w⋅x\n","i\n","​\n"," +b)≥1−ξ\n","i\n","​\n","\n","\n","and\n","𝜉\n","𝑖\n","≥\n","0\n","ξ\n","i\n","​\n"," ≥0, where:\n","\n","∥\n","𝑤\n","∥\n","2\n","∥w∥\n","2\n","  is the regularization term that penalizes the model's complexity,\n","𝜉\n","𝑖\n","ξ\n","i\n","​\n","  are slack variables that allow for misclassifications,\n","𝐶\n","C is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error,\n","𝑦\n","𝑖\n","y\n","i\n","​\n","  are the class labels.\n","Q3. What is the Kernel Trick in SVM?\n","The kernel trick is a technique used in SVMs to transform the original feature space into a higher-dimensional space where the data may become more easily separable by a linear hyperplane. This is done without explicitly computing the coordinates of the data in the high-dimensional space. Instead, it uses a kernel function\n","𝐾\n","(\n","𝑥\n",",\n","𝑥\n","′\n",")\n","K(x,x\n","′\n"," ) to compute the inner products in the transformed space directly.\n","\n","Common kernel functions include:\n","\n","Linear kernel:\n","𝐾\n","(\n","𝑥\n",",\n","𝑥\n","′\n",")\n","=\n","𝑥\n","⋅\n","𝑥\n","′\n","K(x,x\n","′\n"," )=x⋅x\n","′\n","\n","Polynomial kernel:\n","𝐾\n","(\n","𝑥\n",",\n","𝑥\n","′\n",")\n","=\n","(\n","𝑥\n","⋅\n","𝑥\n","′\n","+\n","𝑐\n",")\n","𝑑\n","K(x,x\n","′\n"," )=(x⋅x\n","′\n"," +c)\n","d\n","\n","Radial basis function (RBF) kernel:\n","𝐾\n","(\n","𝑥\n",",\n","𝑥\n","′\n",")\n","=\n","exp\n","⁡\n","(\n","−\n","𝛾\n","∥\n","𝑥\n","−\n","𝑥\n","′\n","∥\n","2\n",")\n","K(x,x\n","′\n"," )=exp(−γ∥x−x\n","′\n"," ∥\n","2\n"," )\n","Q4. The Role of Support Vectors in SVM\n","Support vectors are the data points that are closest to the decision boundary (or hyperplane) in the feature space. These points are critical in defining the position and orientation of the hyperplane, as they are the ones that contribute to the margin calculation. The margin is the distance between the hyperplane and the nearest support vectors.\n","\n","Example:\n","Consider a simple 2D binary classification problem where data points are distributed in two classes. The support vectors are the points that lie closest to the decision boundary on either side. These points determine the maximum margin hyperplane that separates the classes.\n","\n","Q5. Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM\n","Hyperplane: The decision boundary that separates the data into different classes. In a linear SVM, it is a flat plane in the feature space.\n","\n","Marginal Plane: The planes that are parallel to the hyperplane and pass through the support vectors. These define the boundaries of the margin.\n","\n","Soft Margin: Allows some misclassifications (data points within the margin or on the wrong side) by introducing slack variables (\n","𝜉\n","𝑖\n","ξ\n","i\n","​\n"," ). This is controlled by the parameter\n","𝐶\n","C, which balances the margin size and the classification error.\n","\n","Hard Margin: Assumes no misclassifications and that all data points are correctly classified and outside the margin. It is used when data is linearly separable.\n","\n","Q6. SVM Implementation with the Iris Dataset\n","Let's implement an SVM using the Iris dataset with the scikit-learn library:\n","\n","Load the Iris dataset and split it into training and testing sets:\n","python\n","Copy code\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","Train a linear SVM classifier on the training set and predict the labels for the testing set:\n","python\n","Copy code\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Train a linear SVM classifier\n","clf = SVC(kernel='linear', C=1)\n","clf.fit(X_train, y_train)\n","\n","# Predict the labels for the testing set\n","y_pred = clf.predict(X_test)\n","Compute the accuracy of the model on the testing set:\n","python\n","Copy code\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy:.2f}')\n","Plot the decision boundaries of the trained model using two of the features:\n","To visualize, we will use the first two features for simplicity.\n","\n","python\n","Copy code\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define a function to plot the decision boundaries\n","def plot_decision_boundaries(X, y, model):\n","    h = .02  # step size in the mesh\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.title('Decision boundaries')\n","    plt.show()\n","\n","# Plot decision boundaries using the first two features\n","plot_decision_boundaries(X_train[:, :2], y_train, clf)\n","Try different values of the regularization parameter\n","𝐶\n","C and see how it affects the performance of the model:\n","python\n","Copy code\n","# Train SVM with different values of C\n","C_values = [0.01, 0.1, 1, 10, 100]\n","for C in C_values:\n","    clf = SVC(kernel='linear', C=C)\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f'C={C}, Accuracy: {accuracy:.2f}')\n","In this code, you can observe how changing the value of\n","𝐶\n","C affects the model's performance. Smaller\n","𝐶\n","C values create a wider margin with more misclassifications, while larger\n","𝐶\n","C values aim for fewer misclassifications but with a narrower margin.\n","\n","These steps demonstrate the implementation and evaluation of an SVM classifier using the Iris dataset, showcasing the impact of different regularization parameters on model performance.\n","\n","\n","\n","\n","\n","\n","4o"],"metadata":{"id":"huOn0UeFdxQa"}},{"cell_type":"code","source":[],"metadata":{"id":"ePNQ_JcId8af"},"execution_count":null,"outputs":[]}]}