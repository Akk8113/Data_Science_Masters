{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhntQOlCDuKtGsKZwjIOXT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. Difference Between Euclidean and Manhattan Distance Metrics\n","\n","Euclidean Distance: Measures the straight-line distance between two points in the feature space. It is calculated as\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","(\n","𝑥\n","𝑖\n","−\n","𝑦\n","𝑖\n",")\n","2\n","∑\n","i=1\n","n\n","​\n"," (x\n","i\n","​\n"," −y\n","i\n","​\n"," )\n","2\n","\n","​\n"," . This metric is suitable for cases where you assume the data is distributed in a continuous space and distances between points are important.\n","\n","Manhattan Distance: Measures the distance between two points by summing the absolute differences of their coordinates. It is calculated as\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","∣\n","𝑥\n","𝑖\n","−\n","𝑦\n","𝑖\n","∣\n","∑\n","i=1\n","n\n","​\n"," ∣x\n","i\n","​\n"," −y\n","i\n","​\n"," ∣. This metric is more appropriate for cases where you are dealing with a grid-like path or the data has discrete steps.\n","\n","Effect on Performance:\n","\n","Euclidean distance can be sensitive to the magnitude of the features and can be skewed by features with larger ranges. It might work better when features have similar ranges and units.\n","Manhattan distance can be more robust in high-dimensional spaces or when features have different scales, but it may not always capture the true similarity if the data is inherently continuous.\n","Q2. Choosing the Optimal Value of k\n","\n","Cross-Validation: Split the training data into several folds, train the model on some folds, and validate on others. This helps in selecting the k that performs best on unseen data.\n","\n","Grid Search: Systematically test a range of k values and evaluate their performance using cross-validation.\n","\n","Error Analysis: Plot the error rate against different k values. Generally, a smaller k can lead to high variance and overfitting, while a larger k can lead to high bias and underfitting. The optimal k balances these aspects.\n","\n","Q3. Impact of Distance Metric on Performance\n","\n","The choice of distance metric affects how distances between points are computed, which in turn affects how neighbors are identified and weighted.\n","\n","Euclidean Distance: Suitable for continuous and similar scale features. It assumes a spherical distribution of data points and may perform better when the data is spread out uniformly.\n","\n","Manhattan Distance: Useful for high-dimensional data and discrete features. It can be more robust to outliers and noisy data.\n","\n","Situations to Choose One Over the Other:\n","\n","Use Euclidean distance when features are continuous and have similar scales.\n","Use Manhattan distance in cases where features are discrete or the data is high-dimensional.\n","Q4. Common Hyperparameters in KNN\n","\n","k: Number of neighbors to consider. Tuning is essential to balance bias and variance.\n","\n","Distance Metric: Choosing between Euclidean, Manhattan, or other distance metrics based on the nature of the data.\n","\n","Weight Function: Determines if all neighbors are equally weighted or if nearer neighbors have more influence (e.g., uniform or distance-based weighting).\n","\n","Tuning Techniques:\n","\n","Use cross-validation to evaluate the impact of different hyperparameters.\n","Employ grid search or randomized search to find the best combination of hyperparameters.\n","Q5. Size of the Training Set\n","\n","Larger training sets can provide a more comprehensive representation of the data distribution, improving the model’s performance and generalization.\n","\n","Techniques to Optimize:\n","\n","Sampling: If training data is too large, consider techniques like stratified sampling or down-sampling to balance the dataset.\n","Dimensionality Reduction: Reduce feature space to make training more efficient.\n","Regularization: Helps in managing larger datasets by penalizing complex models.\n","Q6. Potential Drawbacks of KNN\n","\n","Computational Cost: KNN can be slow for large datasets since it requires computing distances to all training examples.\n","\n","Scalability: KNN does not scale well with high-dimensional data (curse of dimensionality).\n","\n","Sensitivity to Noise: Outliers and noisy data points can significantly affect the performance of KNN.\n","\n","Overcoming Drawbacks:\n","\n","Dimensionality Reduction: Use techniques like PCA to reduce the number of features.\n","Efficient Data Structures: Implement KD-trees or ball-trees for faster neighbor search.\n","Preprocessing: Normalize or standardize features to ensure they are on the same scale and reduce the impact of outliers.\n","Let me know if you need more details on any of these points!"],"metadata":{"id":"_k_AFQmGHVoT"}},{"cell_type":"code","source":[],"metadata":{"id":"O8GIFfOqHZsg"},"execution_count":null,"outputs":[]}]}