{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMenWUf3Yb3kLycyMO4QY1+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. What is an ensemble technique in machine learning?\n","An ensemble technique in machine learning involves combining multiple models to create a single, more robust model. The idea is that by aggregating the predictions of several models, the ensemble model can achieve better performance and generalization than any individual model.\n","\n","Q2. Why are ensemble techniques used in machine learning?\n","Ensemble techniques are used in machine learning for several reasons:\n","\n","Improved Accuracy: By combining multiple models, ensembles can reduce errors and improve prediction accuracy.\n","Robustness: Ensembles tend to be more robust against overfitting, especially when combining diverse models.\n","Stability: Aggregating the predictions of several models can lead to more stable and reliable results.\n","Variance Reduction: Ensemble methods can reduce the variance of predictions, which is particularly useful in complex datasets.\n","Q3. What is bagging?\n","Bagging, or Bootstrap Aggregating, is an ensemble technique that involves training multiple instances of the same learning algorithm on different subsets of the training data and then averaging their predictions (for regression) or taking a majority vote (for classification). Each subset is created by randomly sampling with replacement from the original dataset.\n","\n","Q4. What is boosting?\n","Boosting is an ensemble technique that combines multiple weak learners to form a strong learner. It works by training models sequentially, with each model trying to correct the errors of its predecessor. The predictions of all models are then combined through a weighted sum (for regression) or majority vote (for classification). Popular boosting algorithms include AdaBoost and Gradient Boosting.\n","\n","Q5. What are the benefits of using ensemble techniques?\n","The benefits of using ensemble techniques include:\n","\n","Improved Performance: They often achieve higher accuracy than individual models.\n","Reduced Overfitting: They tend to generalize better to new data.\n","Increased Stability: They are less sensitive to noise and fluctuations in the data.\n","Flexibility: They can combine different types of models to leverage their strengths.\n","Q6. Are ensemble techniques always better than individual models?\n","No, ensemble techniques are not always better than individual models. While they generally improve performance and robustness, they can also introduce complexity and require more computational resources. Additionally, if the base models are not sufficiently diverse or if the ensemble is not well-constructed, the improvement may be negligible or even negative.\n","\n","Q7. How is the confidence interval calculated using bootstrap?\n","The confidence interval using bootstrap is calculated by repeatedly resampling the data with replacement to create many bootstrap samples, computing the statistic of interest (e.g., mean) for each sample, and then determining the appropriate percentiles of the bootstrap distribution of the statistic. For a 95% confidence interval, the 2.5th and 97.5th percentiles of the bootstrap distribution are typically used.\n","\n","Q8. How does bootstrap work and what are the steps involved in bootstrap?\n","Bootstrap works by creating multiple resamples of the original dataset, with replacement, to estimate the sampling distribution of a statistic. The steps involved in bootstrap are:\n","\n","Resampling: Randomly sample the original dataset with replacement to create a new dataset (bootstrap sample) of the same size.\n","Statistic Calculation: Calculate the statistic of interest (e.g., mean, median) for the bootstrap sample.\n","Repetition: Repeat the resampling and statistic calculation process many times (e.g., 1,000 or 10,000 times) to build a distribution of the statistic.\n","Confidence Interval: Use the bootstrap distribution to estimate the confidence interval by selecting the appropriate percentiles (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval).\n","Q9. Estimate the 95% confidence interval for the population mean height using bootstrap\n","Given:\n","\n","Sample mean height\n","ùë•\n","Àâ\n","=\n","15\n","x\n","Àâ\n"," =15 meters\n","Sample standard deviation\n","ùë†\n","=\n","2\n","s=2 meters\n","Sample size\n","ùëõ\n","=\n","50\n","n=50\n","Let's use bootstrap to estimate the 95% confidence interval.\n","\n","python\n","Copy code\n","import numpy as np\n","\n","# Given data\n","mean_height = 15\n","std_dev = 2\n","n = 50\n","\n","# Simulate the sample data assuming normal distribution\n","np.random.seed(42)\n","sample_data = np.random.normal(loc=mean_height, scale=std_dev, size=n)\n","\n","# Number of bootstrap samples\n","n_bootstrap_samples = 10000\n","bootstrap_means = np.empty(n_bootstrap_samples)\n","\n","# Bootstrap sampling\n","for i in range(n_bootstrap_samples):\n","    bootstrap_sample = np.random.choice(sample_data, size=n, replace=True)\n","    bootstrap_means[i] = np.mean(bootstrap_sample)\n","\n","# Calculate the 95% confidence interval\n","lower_bound = np.percentile(bootstrap_means, 2.5)\n","upper_bound = np.percentile(bootstrap_means, 97.5)\n","\n","(lower_bound, upper_bound)\n","Running the above code will provide the 95% confidence interval for the population mean height. Let's calculate this."],"metadata":{"id":"nhBZRnXaCdiM"}},{"cell_type":"code","source":[],"metadata":{"id":"ZPfpxo0bCjdf"},"execution_count":null,"outputs":[]}]}