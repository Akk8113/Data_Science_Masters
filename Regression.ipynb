{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPA/cNx4oVuWnDx8ATo/LJP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1: Simple Linear Regression vs. Multiple Linear Regression\n","Simple Linear Regression:\n","\n","Definition: A statistical method used to model the relationship between a single independent variable and a dependent variable using a linear equation.\n","Example: Predicting a person's weight (dependent variable) based on their height (independent variable). The model would be of the form\n","Weight\n","=\n","𝛽\n","0\n","+\n","𝛽\n","1\n","×\n","Height\n","+\n","𝜖\n","Weight=β\n","0\n","​\n"," +β\n","1\n","​\n"," ×Height+ϵ.\n","Multiple Linear Regression:\n","\n","Definition: A statistical method used to model the relationship between multiple independent variables and a dependent variable using a linear equation.\n","Example: Predicting a person's weight based on their height, age, and gender. The model would be of the form\n","Weight\n","=\n","𝛽\n","0\n","+\n","𝛽\n","1\n","×\n","Height\n","+\n","𝛽\n","2\n","×\n","Age\n","+\n","𝛽\n","3\n","×\n","Gender\n","+\n","𝜖\n","Weight=β\n","0\n","​\n"," +β\n","1\n","​\n"," ×Height+β\n","2\n","​\n"," ×Age+β\n","3\n","​\n"," ×Gender+ϵ.\n","Q2: Assumptions of Linear Regression\n","Linearity: The relationship between the independent and dependent variables is linear.\n","\n","Check: Use scatter plots and residual plots to examine if the relationship appears linear.\n","Independence: Observations are independent of each other.\n","\n","Check: Analyze residuals to ensure there is no autocorrelation (e.g., using Durbin-Watson test).\n","Homoscedasticity: The variance of residuals is constant across all levels of the independent variables.\n","\n","Check: Use residual plots to check if residuals are evenly spread around zero.\n","Normality of Residuals: Residuals of the model are normally distributed.\n","\n","Check: Use Q-Q plots or histograms of residuals to assess normality.\n","No multicollinearity: Independent variables are not too highly correlated.\n","\n","Check: Use Variance Inflation Factor (VIF) to detect multicollinearity.\n","Q3: Interpreting Slope and Intercept\n","Intercept (\n","𝛽\n","0\n","β\n","0\n","​\n"," ): The value of the dependent variable when all independent variables are zero.\n","\n","Example: In predicting weight from height, if the intercept is 50 kg, it suggests the weight is 50 kg when height is zero (though practically, this may not be meaningful).\n","Slope (\n","𝛽\n","1\n","β\n","1\n","​\n"," ): The change in the dependent variable for each one-unit change in the independent variable.\n","\n","Example: If the slope is 0.5, for each additional centimeter in height, weight increases by 0.5 kg.\n","Q4: Gradient Descent\n","Definition: An optimization algorithm used to minimize the cost function in machine learning models by iteratively updating model parameters to find the minimum of the cost function.\n","\n","Process: Start with initial guesses for parameters, calculate the gradient of the cost function with respect to parameters, and update the parameters in the direction that reduces the cost function.\n","\n","Usage: In training linear regression models, gradient descent adjusts the weights to minimize the difference between predicted and actual values (cost function).\n","\n","Q5: Multiple Linear Regression\n","Definition: An extension of simple linear regression that uses multiple independent variables to predict a dependent variable. The model is of the form:\n","𝑌\n","=\n","𝛽\n","0\n","+\n","𝛽\n","1\n","𝑋\n","1\n","+\n","𝛽\n","2\n","𝑋\n","2\n","+\n","…\n","+\n","𝛽\n","𝑛\n","𝑋\n","𝑛\n","+\n","𝜖\n","Y=β\n","0\n","​\n"," +β\n","1\n","​\n"," X\n","1\n","​\n"," +β\n","2\n","​\n"," X\n","2\n","​\n"," +…+β\n","n\n","​\n"," X\n","n\n","​\n"," +ϵ\n","\n","Difference from Simple Linear Regression:\n","\n","Simple linear regression involves only one independent variable, while multiple linear regression includes multiple independent variables.\n","Q6: Multicollinearity\n","Definition: Occurs when independent variables in a regression model are highly correlated with each other, making it difficult to estimate the coefficients of the independent variables accurately.\n","\n","Detection:\n","\n","Correlation Matrix: High correlations between independent variables.\n","Variance Inflation Factor (VIF): Values greater than 10 indicate high multicollinearity.\n","Addressing Multicollinearity:\n","\n","Remove or combine correlated variables.\n","Use Principal Component Analysis (PCA) to reduce dimensionality.\n","Q7: Polynomial Regression\n","Definition: A form of regression analysis in which the relationship between the independent variable and dependent variable is modeled as an\n","𝑛\n","nth-degree polynomial.\n","\n","Difference from Linear Regression: While linear regression fits a straight line, polynomial regression fits a curve (e.g., quadratic or cubic).\n","\n","Example: Modeling the relationship between temperature and ice cream sales, where a quadratic model might capture seasonal trends better than a linear model.\n","\n","Q8: Advantages and Disadvantages of Polynomial Regression\n","Advantages:\n","\n","Capture Non-Linearity: Can model complex, non-linear relationships between variables.\n","Flexibility: Allows for fitting curves to data that linear regression cannot.\n","Disadvantages:\n","\n","Overfitting: High-degree polynomials may overfit the data, capturing noise as if it were a pattern.\n","Complexity: Models become more complex and harder to interpret with increasing polynomial degree.\n","When to Use:\n","\n","When data shows a clear non-linear relationship and simple linear regression does not fit well."],"metadata":{"id":"15b1sOes3zPz"}}]}