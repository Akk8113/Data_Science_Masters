{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+TXo/yLw6FaDnF5yTkG1P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","Q1. Concept of R-squared\n","R-squared: Represents the proportion of variance in the dependent variable that is predictable from the independent variables.\n","Calculation:\n","𝑅\n","2\n","=\n","1\n","−\n","Sum of Squared Residuals (SSR)\n","Total Sum of Squares (SST)\n","R\n","2\n"," =1−\n","Total Sum of Squares (SST)\n","Sum of Squared Residuals (SSR)\n","​\n","\n","\n","Where:\n","SSR =\n","∑\n","(\n","𝑦\n","𝑖\n","−\n","𝑦\n","^\n","𝑖\n",")\n","2\n","∑(y\n","i\n","​\n"," −\n","y\n","^\n","​\n","  \n","i\n","​\n"," )\n","2\n","\n","SST =\n","∑\n","(\n","𝑦\n","𝑖\n","−\n","𝑦\n","ˉ\n",")\n","2\n","∑(y\n","i\n","​\n"," −\n","y\n","ˉ\n","​\n"," )\n","2\n","\n","Interpretation: An\n","𝑅\n","2\n","R\n","2\n","  value of 0.80 means 80% of the variance in the dependent variable is explained by the model, while 20% is unexplained.\n","Q2. Adjusted R-squared\n","Definition: Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. It penalizes excessive use of non-informative predictors.\n","Calculation:\n","Adjusted\n","𝑅\n","2\n","=\n","1\n","−\n","(\n","1\n","−\n","𝑅\n","2\n","𝑛\n","−\n","𝑝\n","−\n","1\n",")\n","×\n","(\n","𝑛\n","−\n","1\n",")\n","Adjusted R\n","2\n"," =1−(\n","n−p−1\n","1−R\n","2\n","\n","​\n"," )×(n−1)\n","\n","Where:\n","𝑛\n","n = number of observations\n","𝑝\n","p = number of predictors\n","Difference: While\n","𝑅\n","2\n","R\n","2\n","  can increase with more predictors, adjusted\n","𝑅\n","2\n","R\n","2\n","  can decrease if the added predictors do not improve the model sufficiently.\n","Q3. When to Use Adjusted R-squared\n","Use Adjusted R-squared: When comparing models with different numbers of predictors, as it provides a more accurate measure of model performance by accounting for the number of predictors.\n","Q4. RMSE, MSE, and MAE\n","RMSE (Root Mean Squared Error):\n","Calculation:\n","1\n","𝑛\n","∑\n","(\n","𝑦\n","𝑖\n","−\n","𝑦\n","^\n","𝑖\n",")\n","2\n","n\n","1\n","​\n"," ∑(y\n","i\n","​\n"," −\n","y\n","^\n","​\n","  \n","i\n","​\n"," )\n","2\n","\n","​\n","\n","Represents: Average magnitude of errors, penalizes large errors more than smaller errors.\n","MSE (Mean Squared Error):\n","Calculation:\n","1\n","𝑛\n","∑\n","(\n","𝑦\n","𝑖\n","−\n","𝑦\n","^\n","𝑖\n",")\n","2\n","n\n","1\n","​\n"," ∑(y\n","i\n","​\n"," −\n","y\n","^\n","​\n","  \n","i\n","​\n"," )\n","2\n","\n","Represents: Average of the squared differences between observed and predicted values.\n","MAE (Mean Absolute Error):\n","Calculation:\n","1\n","𝑛\n","∑\n","∣\n","𝑦\n","𝑖\n","−\n","𝑦\n","^\n","𝑖\n","∣\n","n\n","1\n","​\n"," ∑∣y\n","i\n","​\n"," −\n","y\n","^\n","​\n","  \n","i\n","​\n"," ∣\n","Represents: Average of the absolute differences between observed and predicted values.\n","Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n","RMSE:\n","\n","Advantages: Sensitive to large errors, giving a more penalized view of model performance.\n","Disadvantages: Less robust to outliers; error magnitudes are squared, which can exaggerate the impact of outliers.\n","MSE:\n","\n","Advantages: Penalizes large errors more than MAE, providing insight into the variance of the errors.\n","Disadvantages: Sensitive to outliers due to squaring of errors.\n","MAE:\n","\n","Advantages: Provides a straightforward measure of average error, less sensitive to outliers compared to RMSE and MSE.\n","Disadvantages: Does not penalize large errors as heavily as RMSE.\n","Q6. Lasso Regularization\n","Lasso Regularization:\n","Concept: Adds a penalty equal to the absolute value of the magnitude of coefficients (\n","𝜆\n","∑\n","∣\n","𝛽\n","𝑗\n","∣\n","λ∑∣β\n","j\n","​\n"," ∣).\n","Difference from Ridge: Lasso can reduce some coefficients to zero, effectively performing feature selection. Ridge regularization adds a penalty equal to the square of the magnitude of coefficients (\n","𝜆\n","∑\n","𝛽\n","𝑗\n","2\n","λ∑β\n","j\n","2\n","​\n"," ), but does not set coefficients to zero.\n","Appropriate Use: When you suspect that some features are not useful and you want to perform feature selection.\n","Q7. Preventing Overfitting with Regularized Models\n","Concept: Regularized models add a penalty term to the loss function to constrain the model complexity, which helps in reducing overfitting by discouraging overly complex models.\n","Example: In a linear regression model with many features, applying Lasso or Ridge regularization can prevent the model from fitting the noise in the training data, thus improving its generalization to new data.\n","Q8. Limitations of Regularized Linear Models\n","Limitations:\n","May not perform well if the true relationship is not linear.\n","Lasso may exclude important features if not properly tuned.\n","Ridge regularization does not perform feature selection and may still include many irrelevant features.\n","Q9. Choosing Between Models Using RMSE and MAE\n","Comparison:\n","Model A (RMSE = 10) and Model B (MAE = 8): If minimizing large errors is crucial, Model A might be preferred due to its sensitivity to large deviations. Otherwise, if you want a more straightforward measure of\n"],"metadata":{"id":"2avcbbwB4R7B"}}]}